{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1 \n",
    "- Using Enron data-set, perform 3 analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis 1\n",
    "- Let's analyse the data of all the employees sent mails and check how many mail communication was gone through on yearly basis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import all the packages needed for the implementation\n",
    "import glob\n",
    "import os\n",
    "import stop_words\n",
    "import re\n",
    "import email.utils\n",
    "from operator import itemgetter\n",
    "from os.path import isfile,isdir\n",
    "from email.parser import Parser\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# getting the relative path for the folder of the data\n",
    "path = os.getcwd()\n",
    "rootdir = os.path.join(os.getcwd(),'maildir')\n",
    "pathOfDirector = os.walk(rootdir)\n",
    "directories =[direc[0] for direc in pathOfDirector]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this function returns all the sent paths of all the employees folders\n",
    "def sentMailsPathList(directories):\n",
    "    sentPath = []\n",
    "    for directory in directories:\n",
    "        if directory.endswith('_sent_mail'):\n",
    "            sentPath.append(directory)\n",
    "    return sentPath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sentpaths = sentMailsPathList(directories) # calling the above function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# yearly wise mails count is calculated to check which year got high mails\n",
    "def yearlyMails(sentpaths):\n",
    "    years_email = {}\n",
    "    yearList = []\n",
    "    for directory in sentpaths: # iterating through the sent paths\n",
    "        sentMails = glob.glob(directory+ '\\\\' + '*') # getting all the files\n",
    "        for mail in sentMails: # iterating through the files\n",
    "            if os.path.isfile(mail): \n",
    "                with open(mail,'r') as text_file: # opening each file\n",
    "                    email_data = text_file.read() # reading the file\n",
    "                    email_data= Parser().parsestr(email_data) # reading the file as an email\n",
    "                    email_date = email.utils.parsedate(email_data['date']) # checking the date of email\n",
    "                    yearList.append(email_date[0]) # appending date to the list \n",
    "                    year = email_date[0] # getting the year\n",
    "                    month = email_date[1] # getting the date\n",
    "                    if year not in years_email: # iterating through the years\n",
    "                        years_email[year] = {} # adding an empty dictionary\n",
    "                        years_email[year][month] = 1 #\n",
    "                        years_email[year]['counts'] = 1\n",
    "                    else:\n",
    "                        years_email[year]['counts'] += 1 # adding the counter\n",
    "                        if month not in years_email[year]:\n",
    "                            years_email[year][month] = 1\n",
    "    return(years_email) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{2000: {1: 1, 2: 1, 3: 1, 4: 1, 5: 1, 6: 1, 7: 1, 8: 1, 9: 1, 10: 1, 11: 1, 12: 1, 'counts': 17421}, 2001: {1: 1, 2: 1, 3: 1, 4: 1, 5: 1, 6: 1, 7: 1, 8: 1, 9: 1, 10: 1, 11: 1, 'counts': 11634}, 2002: {'counts': 22, 6: 1, 7: 1}, 1999: {12: 1, 'counts': 346}}\n"
     ]
    }
   ],
   "source": [
    "years= yearlyMails(sentpaths)\n",
    "print(years)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis 2\n",
    "- From the above list we can conclude that\n",
    "    - for 2000 year, months 6 to 12 has highest mails\n",
    "    - for 2001 year, months 1 to 5 has highest mails\n",
    "- With this analysis we proceed further by analysising who are the persons involved in those highest mails "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Getting the months which has highest emails\n",
    "year2000Months = [6, 7, 8, 9, 10, 11, 12]\n",
    "year2001Months = [1,2,3,4,5]\n",
    "for year in years.items():\n",
    "    if year[0] == 2000:\n",
    "        for months in year[1].items():\n",
    "            if months[1] > 1000:\n",
    "                if months[0] != 'counts':\n",
    "                    print(\"!count\")\n",
    "                    year2000Months.append(months[0])\n",
    "                    print(year2000Months)\n",
    "    if year[0] == 2001:\n",
    "        for months in year[1].items():\n",
    "            if months[1] > 1000:\n",
    "                if months[0] != 'counts':\n",
    "                    year2001Months.append(months[0])\n",
    "                    print(year2001Months)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "year 2000, months with highest mails: [6, 7, 8, 9, 10, 11, 12]\n",
      "year 2001, months with highest mails: [1, 2, 3, 4, 5]\n"
     ]
    }
   ],
   "source": [
    "print(\"year 2000, months with highest mails:\", year2000Months)\n",
    "print(\"year 2001, months with highest mails:\", year2001Months)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def emailsSentByPersons(sentpaths,year2000Months,year2001Months):\n",
    "    emailSendersReceivers ={}\n",
    "    email_subject = ''\n",
    "    for directory in sentpaths:\n",
    "            sentMails = glob.glob(directory+ '\\\\' + '*')       \n",
    "            for mail in sentMails:\n",
    "                if os.path.isfile(mail): \n",
    "                    with open(mail,'r') as text_file:\n",
    "                        email_data = text_file.read()\n",
    "                        email_data= Parser().parsestr(email_data)\n",
    "                        email_date = email.utils.parsedate(email_data['date'])\n",
    "                        year = email_date[0]\n",
    "                        month = email_date[1]\n",
    "                        month_2000 = year2000Months\n",
    "                        month_2001 = year2001Months\n",
    "                        if (year == 2000 and month in month_2000) or (year == 2001 and month in month_2001):\n",
    "                            email_subject = email_subject + email_data['subject']\n",
    "                            email_to = email_data['to']\n",
    "                            email_from = email_data['from']\n",
    "                            if email_from not in emailSendersReceivers:\n",
    "                                emailSendersReceivers[email_from] = {}\n",
    "                                if email_to is not None:\n",
    "                                    if',' in email_to:\n",
    "                                        email_to.replace('/n','')\n",
    "                                        email_to_list = email_to.split(',')\n",
    "                                        email_to_1 = []\n",
    "                                        for x in email_to_list:\n",
    "                                            x = x.strip()\n",
    "                                            email_to_1.append(x)\n",
    "                                        for emails in email_to_1:\n",
    "                                            if emails not in emailSendersReceivers[email_from]:\n",
    "                                                emailSendersReceivers[email_from][emails] = 1\n",
    "                                            else:\n",
    "                                                emailSendersReceivers[email_from][emails] += 1\n",
    "                                    else:\n",
    "                                        emailSendersReceivers[email_from][email_to] = 1\n",
    "                                emailSendersReceivers[email_from]['counts'] = 1\n",
    "                            else:\n",
    "                                emailSendersReceivers[email_from]['counts'] += 1\n",
    "                                if email_to is not None:\n",
    "                                    if',' in email_to:\n",
    "                                        email_to.replace('/n','')\n",
    "                                        email_to_list = email_to.split(',')\n",
    "                                        email_to_1 = []\n",
    "                                        for x in email_to_list:\n",
    "                                            x = x.strip()\n",
    "                                            email_to_1.append(x)\n",
    "                                        for emails in email_to_1:\n",
    "                                            if emails not in emailSendersReceivers[email_from]:\n",
    "                                                emailSendersReceivers[email_from][emails] = 1\n",
    "                                            else:\n",
    "                                                emailSendersReceivers[email_from][emails] += 1\n",
    "                                    else:\n",
    "                                        if email_to not in emailSendersReceivers[email_from]:\n",
    "                                            emailSendersReceivers[email_from][email_to] = 1\n",
    "                                        else:\n",
    "                                            emailSendersReceivers[email_from][email_to] += 1\n",
    "                                        \n",
    "    return emailSendersReceivers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "numberMailsSent = emailsSentByPersons(sentpaths,year2000Months,year2001Months)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 2 persons who's communication are high:\n",
      "\n",
      "kay.mann@enron.com : 4096 \n",
      "\n",
      "vince.kaminski@enron.com : 2589\n"
     ]
    }
   ],
   "source": [
    "countOfPersonsMails = []\n",
    "for persons in numberMailsSent.items():\n",
    "        personsCount = []\n",
    "        personsCount.append(persons[0])\n",
    "        personsCount.append(persons[1]['counts'])\n",
    "        countOfPersonsMails.append(personsCount)\n",
    "sortedCountOfPersonMails = sorted(countOfPersonsMails, key=itemgetter(1), reverse = True)\n",
    "top2 = sortedCountOfPersonMails[:2]\n",
    "print(\"Top 2 persons who's communication are high:\\n\")\n",
    "print(top2[0][0],\":\",top2[0][1], '\\n')\n",
    "print(top2[1][0],\":\",top2[1][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis 3\n",
    "- We analysed that Mann Kay and Vince Kaaminski has communicated more \n",
    "\n",
    "- Finally lets analyse what they were speaking about in those mails "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mann = []\n",
    "kaminski = []\n",
    "for directory in sentpaths:\n",
    "        sentMails = glob.glob(directory+ '\\\\' + '*')       \n",
    "        for mail in sentMails:\n",
    "            if os.path.isfile(mail): \n",
    "                with open(mail,'r') as text_file:\n",
    "                    email_data = text_file.read()\n",
    "                    email_data= Parser().parsestr(email_data)\n",
    "                    email_date = email.utils.parsedate(email_data['date'])\n",
    "                    year = email_date[0]\n",
    "                    month = email_date[1]\n",
    "                    month_2000 = year2000Months\n",
    "                    month_2001 = year2001Months\n",
    "                    if (year == 2000 and month in month_2000) or (year == 2001 and month in month_2001):\n",
    "                        if email_data['from'] == 'kay.mann@enron.com':\n",
    "                            mann.append(email_data['subject'])\n",
    "                        if email_data['from'] == 'vince.kaminski@enron.com':\n",
    "                            kaminski.append(email_data['subject'])\n",
    "                     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chint\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:855: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "from stop_words import get_stop_words\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from gensim import corpora, models\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import warnings\n",
    "import gensim\n",
    "import urllib.parse\n",
    "warnings.filterwarnings(action='ignore', category=UserWarning, module='gensim')\n",
    "from urllib.parse import urlparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "# create English stop words list\n",
    "en_stop = get_stop_words('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create p_stemmer of class PorterStemmer\n",
    "#p_stemmer = PorterStemmer()\n",
    "wordnet_lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def dataToWriteInCsvKaminski(kaminski):\n",
    "    texts = []\n",
    "    listOfWords = ['forwarded', 'subject', 'mail', 'Original', 'Message', 'will']\n",
    "\n",
    "    for i in kaminski:\n",
    "        raw = i.lower()\n",
    "        http = re.sub(r\"http\\S+\", \":\", raw)\n",
    "        forwardMessage = re.sub(r'(?m)^\\-.*\\n?', \"\", raw)\n",
    "        tagRemoval = re.sub(r'subject.*|to.*|cc.*|from.*|sent by.*|.*@*,.+\\n', \"\", forwardMessage)\n",
    "        tokens = tokenizer.tokenize(tagRemoval)\n",
    "        #print(tokens)\n",
    "        isalpha = [i for i in tokens if  i.isalpha()]\n",
    "        #print(tokens)\n",
    "        shortwords = [word for word in isalpha if len(word)>3]\n",
    "        #print(shortwords)\n",
    "        removeWords = [i for i in shortwords if not i in listOfWords]\n",
    "        # remove stop words from tokens\n",
    "        stopped_tokens = [i for i in removeWords if not i in en_stop]\n",
    "        # stem tokens\n",
    "        stemmed_tokens = [wordnet_lemmatizer.lemmatize(i) for i in stopped_tokens]\n",
    "        # add tokens to list\n",
    "        #print(stemmed_tokens)\n",
    "        texts.append(stemmed_tokens)\n",
    "    # turn our tokenized documents into a id <-> term dictionary\n",
    "    dictionary = corpora.Dictionary(texts)\n",
    "    #print(dictionary.token2id)\n",
    "    # convert tokenized documents into a document-term matrix\n",
    "    corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "    #print(corpus)\n",
    "    ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics=5, id2word = dictionary)\n",
    "    x = ldamodel.print_topics(num_topics=5, num_words=15)\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def dataToWriteInCsvMann(mann):\n",
    "    texts = []\n",
    "    listOfWords = ['forwarded', 'subject', 'mail', 'Original', 'Message', 'will']\n",
    "\n",
    "    for i in mann:\n",
    "        raw = i.lower()\n",
    "        http = re.sub(r\"http\\S+\", \":\", raw)\n",
    "        forwardMessage = re.sub(r'(?m)^\\-.*\\n?', \"\", raw)\n",
    "        tagRemoval = re.sub(r'subject.*|to.*|cc.*|from.*|sent by.*|.*@*,.+\\n', \"\", forwardMessage)\n",
    "        tokens = tokenizer.tokenize(tagRemoval)\n",
    "        #print(tokens)\n",
    "        isalpha = [i for i in tokens if  i.isalpha()]\n",
    "        #print(tokens)\n",
    "        shortwords = [word for word in isalpha if len(word)>3]\n",
    "        #print(shortwords)\n",
    "        removeWords = [i for i in shortwords if not i in listOfWords]\n",
    "        # remove stop words from tokens\n",
    "        stopped_tokens = [i for i in removeWords if not i in en_stop]\n",
    "        # stem tokens\n",
    "        stemmed_tokens = [wordnet_lemmatizer.lemmatize(i) for i in stopped_tokens]\n",
    "        # add tokens to list\n",
    "        #print(stemmed_tokens)\n",
    "        texts.append(stemmed_tokens)\n",
    "    # turn our tokenized documents into a id <-> term dictionary\n",
    "    dictionary = corpora.Dictionary(texts)\n",
    "    #print(dictionary.token2id)\n",
    "    # convert tokenized documents into a document-term matrix\n",
    "    corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "    #print(corpus)\n",
    "    ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics=5, id2word = dictionary)\n",
    "    x = ldamodel.print_topics(num_topics=5, num_words=15)\n",
    "    print(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# function writing the csv file with restuarants details\n",
    "def csvCreation(filename, data):\n",
    "    with open(filename, 'a',encoding='utf_8', errors='ignore') as outcsv: # opening a csv file as english file\n",
    "        #configure writer to write standard csv file\n",
    "        writer = csv.writer(outcsv, delimiter=',', quotechar='|', quoting=csv.QUOTE_MINIMAL, lineterminator='\\n')\n",
    "        # writing the headers for the file\n",
    "        writer.writerow(['TOPICS', 'WORD OCCURENCES PROBABILITY'])\n",
    "        for eachRes in data:\n",
    "            writer.writerow([eachRes[0],eachRes[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# check whether the directory is already created if not this function creates it\n",
    "def dir_check(file_path):\n",
    "    if not os.path.exists(file_path):\n",
    "        os.makedirs(file_path) # creates the complete folder structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# creating the csv file at specified path\n",
    "def output(csvPath2,csvPath1,mann,kaminski):\n",
    "    if os.path.exists(csvPath1):\n",
    "        os.remove(csvPath1) # removing the file if already present\n",
    "        mann = dataToWriteInCsvMann(mann) # creating the restuarants data\n",
    "        csvCreation(csvPath1, mann) # creating the csv file  \n",
    "    else:\n",
    "        mann = dataToWriteInCsvMann(mann) # creating the restuarants data\n",
    "        csvCreation(csvPath1, mann) # creating the csv file\n",
    " \n",
    "    if os.path.exists(csvPath2):\n",
    "        os.remove(csvPath2)\n",
    "        kaminski = dataToWriteInCsvKaminski(kaminski) # creating the restuarants data\n",
    "        csvCreation(csvPath2, kaminski) # creating the csv file\n",
    "    else:\n",
    "        kaminski = dataToWriteInCsvKaminski(kaminski) # creating the restuarants data\n",
    "        csvCreation(csvPath2, kaminski) # creating the csv file\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "csvFolder = os.path.join(path,'Question 1\\Analysis 3')\n",
    "dir_check(csvFolder)\n",
    "csvPath1 = os.path.join(csvFolder,'Q1Analysis3_TopicModelling_Mann.csv') # path is taken\n",
    "csvPath2 = os.path.join(csvFolder,'Q1Analysis3_TopicModelling_Kaminski.csv') # path is taken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, '0.141*\"mdea\" + 0.058*\"committee\" + 0.058*\"proposal\" + 0.038*\"missing\" + 0.034*\"split\" + 0.021*\"pseg\" + 0.020*\"message\" + 0.020*\"conference\" + 0.019*\"power\" + 0.019*\"call\" + 0.019*\"rotation\" + 0.016*\"agreement\" + 0.009*\"delete\" + 0.009*\"previous\" + 0.009*\"letter\"'), (1, '0.108*\"delta\" + 0.069*\"turbine\" + 0.041*\"contract\" + 0.040*\"order\" + 0.034*\"change\" + 0.025*\"issue\" + 0.025*\"favor\" + 0.024*\"piece\" + 0.021*\"document\" + 0.015*\"blue\" + 0.014*\"split\" + 0.014*\"call\" + 0.014*\"enron\" + 0.014*\"list\" + 0.014*\"number\"'), (2, '0.073*\"pegasus\" + 0.066*\"letter\" + 0.047*\"lunch\" + 0.046*\"agreement\" + 0.034*\"marketing\" + 0.028*\"option\" + 0.027*\"city\" + 0.024*\"dinner\" + 0.023*\"execution\" + 0.016*\"contract\" + 0.016*\"update\" + 0.015*\"version\" + 0.015*\"revised\" + 0.014*\"final\" + 0.013*\"compared\"'), (3, '0.133*\"agreement\" + 0.107*\"salmon\" + 0.073*\"energy\" + 0.072*\"turbine\" + 0.060*\"consent\" + 0.056*\"legal\" + 0.030*\"revised\" + 0.019*\"purchase\" + 0.019*\"letter\" + 0.018*\"cogen\" + 0.017*\"austin\" + 0.016*\"resort\" + 0.016*\"lake\" + 0.015*\"celebration\" + 0.015*\"question\"'), (4, '0.116*\"opinion\" + 0.040*\"agreement\" + 0.037*\"vepco\" + 0.035*\"assignment\" + 0.033*\"saturday\" + 0.031*\"transaction\" + 0.025*\"consent\" + 0.024*\"document\" + 0.022*\"comment\" + 0.019*\"purchase\" + 0.019*\"meeting\" + 0.018*\"beach\" + 0.017*\"enron\" + 0.017*\"contract\" + 0.013*\"edgecombe\"')]\n"
     ]
    }
   ],
   "source": [
    "output(csvPath2,csvPath1,mann,kaminski) # calling the function to create csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
